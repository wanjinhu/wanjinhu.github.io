

[
  
  
    {
      "title"    : "页面没有找到",
      "url"      : "http://localhost:4000/404.html",
      "keywords" : "404"
    } ,
  
  
  
    {
      "title"    : "About",
      "url"      : "http://localhost:4000/about/",
      "keywords" : "Wanjin Hu, 胡万金"
    } ,
  
  
  
    {
      "title"    : "归档",
      "url"      : "http://localhost:4000/archives/",
      "keywords" : "归档"
    } ,
  
  
  
    {
      "title"    : "Categories",
      "url"      : "http://localhost:4000/categories/",
      "keywords" : "分类"
    } ,
  
  
  
    {
      "title"    : "捐助 / Donate",
      "url"      : "http://localhost:4000/donate/",
      "keywords" : "Donate"
    } ,
  
  
  
    {
      "title"    : "Fragments",
      "url"      : "http://localhost:4000/fragments/",
      "keywords" : "fragments"
    } ,
  
  
  
  
  
    {
      "title"    : "Links",
      "url"      : "http://localhost:4000/links/",
      "keywords" : "友情链接"
    } ,
  
  
  
    {
      "title"    : "mindmap",
      "url"      : "http://localhost:4000/mindmap-viewer/",
      "keywords" : "mindmap"
    } ,
  
  
  
    {
      "title"    : "Open Source Projects",
      "url"      : "http://localhost:4000/open-source/",
      "keywords" : "开源,open-source,GitHub,开源项目"
    } ,
  
  
  
  
  
    {
      "title"    : "Wiki",
      "url"      : "http://localhost:4000/wiki/",
      "keywords" : "维基，Wiki"
    } ,
  
  
  
  
  
  
  
  
  
  

  
    {
      "title"    : "宏基因组分析流程",
      "category" : "测序数据分析",
      "content": "提供一个宏基因组分析流程。 Here is a metagenomic sequence data analysis pipeline, nothing different with other pipelines. But if you want to get to know the metagenomic analysis pipeline step by step, maybe you can get some details from this repository. And it is suitable for the beginners i think. Scripts and test files you can find here: Metagenomic-Analysis-Pipeline Pipeline overview 🐫  Raw sequence quality trim Host reference sequence remove Metaphlan for composition of microbial communities Sequence assembly Gene prediction Remove redundancy gene and build non-redundant geneset Function annotation using emapper Organize function results table Quick start 🦏 $python pipe_metagenome.py -h usage: ================================================================= python pipe_metagenome.py --fastq_list fq.list --output_dir result --ref ref_bowtie2_index ref_bowtie2_index: canis: /root/database/Canis_GCF_000002285.5/Canis_GCF_000002285_5 human: /root/database/hg38_GCF_000001405.40/GCF_000001405.40/hg38 ================================================================= Pipeline of metagenome optional arguments: -h, --help  show this help message and exit -l FQLIST, --fastq_list FQLIST     raw fq list -o OUTDIR, --output_dir OUTDIR     result output -r REF, --ref REF  ref genome bowtie2 index What you need to do is to provide two input files:  –fastq_list # sample - fq_R1 - fq_R2 list, format like fq.list –ref # host reference genome bowtie2 index And set output dir --output_dir, all of output results would be included. Output files explanation 🐊 Output files tree (not show all files) ├── 00-result     # most important results in this fold │   ├── 00_merged_abundance_table.txt # composition of microbial communities │   ├── 01_metaphlan_phylum.txt  # communities in phylum level │   ├── 02_metaphlan_class.txt  # communities in class level │   ├── 03_metaphlan_order.txt  # communities in order level │   ├── 04_metaphlan_family.txt  # communities in family level │   ├── 05_metaphlan_genus.txt  # communities in genus level │   ├── 06_metaphlan_species.txt  # communities in species level │   ├── KO_samples.xls   # KEGG KO gene composition table │   └── pathway_samples.xls   # KEGG pathway composition table ├── 01-fastp_trim ├── 02-ref_remove ├── 03-metaphlan ├── 04-megahit ├── 05-prodigal ├── 06-cdhit ├── 07-emapper ├── 08-sam_count ├── 09-emapper_kegg Output important result files explanation metaphlan_diff-levels.txt  column 1 : communities information column 2 ~ : communities abundance percent of samples clade_name C1 k__Bacteria;p__Firmicutes 86.51132 k__Bacteria;p__Actinobacteria 6.52203 k__Bacteria;p__Bacteroidetes 4.24729 k__Bacteria;p__Proteobacteria 1.96422 k__Bacteria;p__Fusobacteria 0.75514 k__Bacteria;p__Tenericutes 0.0 k__Bacteria;p__Spirochaetes 0.0 k__Bacteria;p__Verrucomicrobia 0.0 ... KO_samples.xls  column 1 : KO gene name column 2 : KO gene description column 3 : KO gene id column 4 ~ : KO gene number of samples KO_name tKO_des tKO tC1 E1.1.1.1, adh talcohol dehydrogenase [EC:1.1.1.1] tK00001 t2817.0 AKR1A1, adh talcohol dehydrogenase (NADP+) [EC:1.1.1.2] tK00002 t254.0 hom thomoserine dehydrogenase [EC:1.1.1.3] tK00003 t2890.0 BDH, butB t(R,R)-butanediol dehydrogenase / meso-butanediol dehydrogenase / diacetyl reductase [EC:1.1.1.4 1.1.1.- 1.1.1.303] tK00004 t20.0 ... pathway_samples.xls  column 1 : pathway level 1 column 2 : pathway level 2 column 3 : pathway level 3 column 4 : pathway id column 5 ~ : pathway gene number of samples level1 tlevel2 tlevel3 tpathway tC1 Metabolism tCarbohydrate metabolism tGlycolysis / Gluconeogenesis tko00010 t91005.0 Metabolism tCarbohydrate metabolism tCitrate cycle (TCA cycle) tko00020 t31442.0 Metabolism tCarbohydrate metabolism tPentose phosphate pathway tko00030 t53905.0 Metabolism tCarbohydrate metabolism tPentose and glucuronate interconversions tko00040 t21334.0 ... Step by step 🦥 Step1 Raw sequence quality trim using fastp fastp -i sample_1.fastq.gz     -o sample_clean.1.fastq.gz     -I sample_2.fastq.gz     -O sample_clean.2.fastq.gz     -w 8 -h sample.html -j sample.json Step2 Host reference sequence remove bowtie2 -x ref_bowtie2_index -1 sample_clean.1.fastq.gz -2 sample_clean.2.fastq.gz -S sample.sam 2&gt;sample.mapping.log samtools fastq -@ 8 -f 4 sample.sam -1 sample.unmap.1.fastq.gz -2 sample.unmap.2.fastq.gz -s sample.unmap.single.fastq.gz Step3 Metaphlan for composition of microbial communities zcat sample.unmap.1.fastq.gz sample.unmap.2.fastq.gz|metaphlan --input_type fastq --bowtie2out sample_bowtie2.bz2 --output_file sample_metaphlan.tsv --nproc 8 # when you install metaphlan in the system, you will get script 'merge_metaphlan_tables.py', that's for merge different samples metaphlan result in one file, like: merge_metaphlan_tables.py *.tsv &gt; 00_merged_abundance_table.txt grep -E '(p__)|(clade_name)' 00_merged_abundance_table.txt |grep -v 'c__'|sed 's/|/;/g' &gt; 01_metaphlan_phylum.txt grep -E '(c__)|(clade_name)' 00_merged_abundance_table.txt |grep -v 'o__'|sed 's/|/;/g' &gt; 02_metaphlan_class.txt grep -E '(o__)|(clade_name)' 00_merged_abundance_table.txt |grep -v 'f__'|sed 's/|/;/g' &gt; 03_metaphlan_order.txt grep -E '(f__)|(clade_name)' 00_merged_abundance_table.txt |grep -v 'g__'|sed 's/|/;/g' &gt; 04_metaphlan_family.txt grep -E '(g__)|(clade_name)' 00_merged_abundance_table.txt |grep -v 's__'|sed 's/|/;/g' &gt; 05_metaphlan_genus.txt grep -E '(s__)|(clade_name)' 00_merged_abundance_table.txt |grep -v 't__'|sed 's/|/;/g' &gt; 06_metaphlan_species.txt Step4 Sequence assembly and trim contigs which length &lt; 500bp megahit -1 sample.unmap.1.fastq.gz -2 sample.unmap.2.fastq.gz -o sample_megahit --out-prefix sample -t 8 seqkit seq -m 500 sample_megahit/sample.contigs.fa --remove-gaps &gt; sample.contigs_500.fa sed -i 's/&gt;/&gt;sample_/g' sample.contigs_500.fa Step5 Gene prediction using prodigal prodigal -p meta -a sample_prot.faa -m -d sample_nucl.fna -o sample_genes.gff -f gff -s sample.stat -i sample.contigs_500.fa Step6 Remove redundancy gene and build non-redundant geneset cat sample1_prot.faa sample2_prot.faa ... &gt; prot.faa cat sample1_nucl.fna sample2_nucl.fna ... &gt; nucl.fna cd-hit -i prot.faa -o prot_nonerude.faa -c 0.95 -T 8 -n 5 -d 0 -aS 0.9 -g 1 -sc 1 -sf 1 -M 0 grep '&gt;' prot_nonerude.faa|awk -F ' ' '{print $1}'|sed 's/&gt;//g' &gt; prot_nonerude.list seqtk subseq nucl.fna prot_nonerude.list &gt; nucl_nonerude.fna bwa index nucl_nonerude.fna -p geneset_bwa bioawk -c fastx '{print $name, length($seq)}' nucl_nonerude.fna &gt; geneset_length.txt Step7 Function annotation using emapper # when you install emapper in the system, you will get script 'emapper.py' emapper.py -i prot_nonerude.faa -o eggnog --cpu 0 --usemem cut -f1,12 eggnog.emapper.annotations|grep -v ^#|sed 's/ko://g'|sed '1i gene  tko'|grep -v - &gt; KEGG_KO.txt cut -f1,13 eggnog.emapper.annotations|grep -v ^#|sed '1i gene  tpathway'|grep -v - &gt; KEGG_PATHWAY.txt Step8 Gene num count bwa mem -t 4 geneset_bwa sample.unmap.1.fastq.gz sample.unmap.2.fastq.gz | samtools view -bS - | samtools sort - &gt; sample_mapping_geneset.bam samtools view -F 4 -F 256 -F 2048 sample_mapping_geneset.bam|awk '{if($3!=*) print $3}'|sort| uniq -c|awk 'BEGIN {FS= ;OFS=,} {print $2,$1}' | awk 'BEGIN {FS=,;OFS=,} {if ($2 &gt; 1) print $1  t$2; else print $1  t0}'|sed '1i gene  tsample' &gt; sample.count Step9 Organize function results table # using /kegg/kegg.py to analysis, like: $python kegg.py -h usage: python kegg.py -kk KEGG_KO.txt -kp KEGG_PATHWAY.txt -mt merged_file.txt -ok out_KO.xls -op out_pathway.xls Merge KO/pathway count table from eggnog result. optional arguments: -h, --help  show this help message and exit -kk KEGGKO, --kegg_KO KEGGKO     Sample's kegg KO information, such as KEGG_KO.txt -kp KEGGPATHWAY, --kegg_pathway KEGGPATHWAY     Sample's kegg pathway information, such as     KEGG_PATHWAY.txt -mt MERGETABLE, --merge_table MERGETABLE     Sample's merged gene count table, such as     merged_file.txt -ok OUTKO, --out_KO OUTKO     Output KO result, such as out_KO.xls -op OUTPATHWAY, --out_pathway OUTPATHWAY     Output pathway result, such as out_pathway.xls -t TMP, --tmp TMP  Tmp files dir Contact 🐖 Wanjin Hu (wanjin.hu@outlook.com) ",
      "url"      : "http://localhost:4000/2023/08/10/pipeline-metagenomic-analysis/",
      "keywords" : "宏基因组，测序数据分析"
    } ,
  
    {
      "title"    : "《人生十二法则》读后的个人思考",
      "category" : "人生意义",
      "content": "真的有12条法则可以来指导你过的人生么？我开始思考自己的人生。 看到书名，你估计可能会想这本书就是一个特傻的鸡汤文，这都什么年代了还搞这一出？这个时代提倡的是个性，是包容，是多元，是自由，是独立，是开放，是博爱。真就掌握了你说的这12条法则就能过好我的人生么？用脚趾头想想都觉得不可能，你了解我是谁么？你知道我经历了什么？你知道我最近有多痛苦么？没人可以定义我的人生，我的人生只能我自己来定义。我有我自己的想法和选择，我和其他人并不一样。所以我压根就懒得听你扯这些大道理，该懂得道理我都懂，我只是做不到。 以上这些想法多少在你的脑海中有过，再加上现在互联网的高度发达，各种社交媒体的包装，还有各种现实问题的困扰，你很难不陷入到这种思维中。平时生活工作总会带着面具，虚伪的对外界展示自己，到了深夜再进行人生思考，各种emo的情绪就会爆发，再影响第二天自己的生活，循环往复。说实话，没有人会不去思考自己的人生，人生存在的目的价值，但是正如上面提到的那些，执拗的去秉持自己的观点，而这些观点甚至不是自己思考获得的，而是各种形式主义灌输给我们的。这样的思考只会让人抑郁和空虚。 人在年轻的时候，既缺乏独立也缺少认知，因为还没来得及积累阅历和智慧来建立起自己的个人标准，所以大多数情况下，只好和他人做比较。大体上，我想人生的成长过程中总会经历这个阶段。如果你不想和他人做比较，就得逐渐建立起自己的标准，有明确的世界观、人生观和价值观。否则一句“我只想做自己，不想和他人比较”之类的话，就是很可笑和没有份量的，可能只是自己逃避的借口。到底这种听起来很酷的自我宣言属于哪一种，恐怕只有自己内心深处明白。我希望不要自欺欺人，至少对自己做到真诚。向内求，不是欺骗自己的空话和逃避的借口，而是符合自我标准的真情实感，这也不是平白无故就可以想明白的，去读书/学习/总结，至少对现在的我来说是一个很好的向求内的路径。 《人生十二法则》是乔丹•皮得森 (Jordan B. Peterson) 教授很早之前写的一本书。了解到皮得森教授是因为他之前的一个访谈类节目的片段（视频链接我会放在最后），他聊到的是一个人在面对撒谎和讲出事实的选择时，为什么要勇敢的讲真话。逃避或者说出真相，不仅仅是两个不同的选择，更是两条人生道路，两种完全不同的存在方式。我需要指出的是，这里讲出事实，不是让你不顾及任何后果的说出真相，而是要明白真相，并做出正确的选择。遇到争议不要害怕讲出自己的观点，依照事实原则，遵循内心的价值观，勇敢表明自己的观点，很有可能你会遭到批判和冷嘲热讽，但是没关系，你每一次的行为就是为塑造你个人价值观的一次巩固。这本书是我探索人生意义上的一本很重要的书，某些时候内心的那种澎湃感，由于自己匮乏的认知和语言能力很难表达出来，看到这本书的时候，我似乎找到了一种表达自己内心想法的途径，书里的某些观点与我的价值观不谋而合，让我觉得自己的价值观是正确的，同时我也可以用这些观点来指导自己，让自己变得更好。 《人生十二法则》的写法很有趣，每条法则都用一个小故事作为引言，也很符合皮得森教授的风格，他本身也是大学的心理学教授(不知道现在还在不在职)，他从几年前就开始在网络上发自己的上课视频，他很擅长用古希腊的神话故事或者其他的故事来引导听众。同时他一直在用严肃认真的态度来对待每个抛给他的问题，我非常欣赏皮得森教授的这种做法，现在这个时代，大家鼓吹的是个性、幽默和个人魅力塑造，以及其他各种无意义和浮于表面的词语，而皮得森教授则用严肃的态度来引导大家思考人生，我觉得这是很有必要的。他的态度和行为都让人感觉很值得学习。 下面是皮得森教授在书中总结的十二条人生法则：  法则一 获胜的龙虾从不低头：笔直站立，昂首挺胸 法则二 像照顾生病的宠物一样关心自己：待己如助人 法则三 摆脱损友：与真心希望你好的人做朋友 法则四 战胜内心的批评家：和昨天的自己比，别和今天的别人比 法则五 管教你家的小怪物：别让孩子做出令你讨厌他的事 法则六 当痛苦到想诅咒一切：批判世界之前先清理你的房间 法则七 苏格拉底的选择：追求意义，拒绝苟且 法则八 不买醉鬼卖的东西：说真话，或者至少别撒谎 法则九 别偷走来访者的问题：假设你聆听的人知道你不知道的事 法则十 不要无视地毯下的龙：直面问题，言辞精确 法则十一 不要打扰玩滑板的孩子们：承认现实，反对偏见 法则十二 当你在街上遇到一只猫时，摸摸它：关注存在的善 批判世界之前先清理你的房间。我觉得这是整本书的基调，在你没有负起对自己人生的责任时，不要过多关注世界的纷扰，而是先整理好自己，让自己变得更有责任感。是的，责任感。现在大家好像在个人生活中不怎么提到这个词了，因为它有点沉重和落于时代，但我觉得还是很重要。就像书中写的，你要成为你父亲葬礼上最值得依靠的人，为你的家人负起责任来。当然有些事情只有经历过之后才会知道，特别是过生活，你想的特别美好的时候，更多是纸上谈兵，我想表明的是这当然不假，这和12条法则并不冲突，甚至这12条法则可以作为你生活工作的指导思想之一。但是你需要提前知道这些信息，提前在你的内心和脑海中种下一颗坚韧的种子，然后把每一次的经历都作为其成长的肥料。罗曼罗兰说过一句话，“生活中只有一种正真的英雄主义，就是认清生活的真相之后依然热爱它”。所以请不要害怕生活中遇到的挫折、困难，明确你自己的人生指导法则，这12条法则只是皮得森教授自己作为临床心理学家的总结，我们可以作为参考，还是需要记录适合自己的人生法则。但是，对我自己来说，责任、坚韧、诚实和追求意义这些存在了很久的人生品质，毫无疑问会作为我人生法则之书的坚实基础。 我还特别想要指出一点的是，这12条法则多数是要求你从自身做起，和昨天的自己比，别和今天的别人比。和别人比较是造成你生活痛苦的重要原因之一，因为这样有了期待、有了嫉妒，你大脑中的多巴胺欲望回路被持续刺激，只想要更多，这样在原本就是痛苦的人生基础上更加的痛苦。还有，这些法则永远要通过你自身的理解，融入到你的生活工作中，用来指导你的人生，千万不要作为你卖弄的资本，想要强加在别人身上，遇到生活过的艰难的人，多给他们报有一点善意吧，不要说那些过得不好是自身不努力的屁话，你的这些诅咒最终都会被你自己反噬。这个世界包含的比你关注的多太多了，你必须认真分配有限的资源。 苏格拉底的选择：追求意义，拒绝苟且。 我的选择：承担责任，追求价值，寻找意义。 相关链接 乔丹•皮得森教授的YouTube主页 豆瓣：人生十二法则 乔丹•皮得森教授分享片段：要敢于表达自我并证明自己 ",
      "url"      : "http://localhost:4000/2023/08/30/reading-12rules-for-life/",
      "keywords" : "人生意义, 阅读"
    } ,
  
    {
      "title"    : "利用细菌全基因组测序方式对菌株安全性评价",
      "category" : "临床数据分析",
      "content": "如何利用全基因组测序方式来评估菌株的安全性？这里主要指抗生素耐药性和毒力因子，以及涉及到的可移动元件。 背景和前提说明 2023年2月13日，国家卫生健康委员会发布了《食品安全国家标准 食品用菌种安全性评价程序》（征求意见稿）。文件你可以点击 这个链接 进行下载。 标准规定了食品用菌种（包括细菌、丝状真菌、酵母、放线菌及单细胞藻类）的安全性评价程序。具体评价方法包括了相关文献综述，全基因组测序，动物致病性试验，耐药性试验，产毒试验和其他的活性代谢产物试验等。应该说评价的内容比较全面，我这里给大家介绍的内容是关于细菌的全基因组测序内容。 这份标准对全基因组测序相关的内容有具体的要求，最主要的是评估菌株基因组序列中可能存在的毒力因子和抗生素耐药性基因，方法就是尽可能的与已知的相关数据库进行比对。标准如下：  序列长度覆盖度≥60%，输入序列与数据库中序列的匹配度（≥ 85 %）和 e 值（≤1e5 ） 文件中虽然没有具体指明比对的软件，上面这3个参数可以利用常见的blast和diamond比对软件获得。 这份标准中提到了具体的数据库，例如毒力基因（或毒素合成关键基因）数据库（包括但不限于 VFDB、PAI DB、MvirDB、CGE 等），耐药基因数据库（包括但不限于CARD、ResFinder、Argannot、NDARO 等）。但是有些数据库多年不更新，不提供基因序列或者只能利用在线比对的方式等，这些数据库就暂时不考虑。原则上就是尽可能的找到毒力和耐药性基因的参考数据库，然后利用比对软件比对，获得比对结果。 参考数据库说明 我这里一共找到12个数据库作为参考，7个抗生素耐药性，3个毒力基因，1个可移动元件和1个原噬菌体。 相同类型数据库中有的基因是重复的，在实际的分析中也体现出来了。同一个基因在不同数据库中记录的信息详细度也是有区别的，但依旧是同一个基因。 实际分析后，比较有效的数据库，抗生素耐药性基因数据库以CARD和MEGARes为代表，毒力因子数据库以VFDB为代表。以下是具体的数据库信息， 抗生素耐药性数据库 CARD : 抗生素耐药性 Resfinder : 抗生素耐药性 NCBI_ARM : NCBI整理的抗生素耐药性数据库 Bacmet2 : 抗生素耐药性 ARG_ANNOT : 抗生素耐药性 MEGARes : 抗生素耐药性 Disinfinder : 消毒剂抗性基因 毒力基因数据库 VFDB : 毒力因子 Virulencefinder : 毒力因子 Ecoli_VF : 来源于大肠杆菌的毒力因子 可移动元件数据库 mobileOG-db : 可移动元件/转录调控因子数据库 原噬菌体 Phigaro : 某些温和的噬菌体侵染细菌后，其核酸整合到宿主细菌染色体中的状态 使用到的软件 Unicycler : 是我用过细菌基因组组装软件中最好的，不论是只有二代测序的基因组扫描图，还是二代+三代测序的基因组完成图。我之前测试过十几款细菌基因组组装软件，可以点击 Microbial-genome 查看相关信息。 GTDBtk : 是根据基因组分类数据库，用来对细菌基因组鉴定物种信息的，我认为是目前基于基因组鉴定物种最好的方法，适用于单菌和宏基因组MAG水平。 PGAP : 是NCBI提供的原核生物基因组注释pipeline，比如Bb-12菌株在NCBI上的信息，可以点击 这个链接 查看，PGAP的注释结果可以作为参考。 Prokka : 可以用来快速注释细菌、古菌和病毒基因组，并且可以生成符合标准的输出文件（比如gff3和gbk等）。 CARD-RGI : 根据同源性（homology）和SNP模型从蛋白质或者核苷酸数据中预测抗生素抗性基因，是利用CARD作为参考数据库的。 BLAST : 对于只提供了核苷酸的数据库，利用BLAST来比对。 DIAMOND : 提供了蛋白的数据库，利用DIAMOND来比对。 Proksee : 基因组分析集合的在线分析网站。这里我用它来绘制基因组圈图，其实网站本身提供了丰富的基因组分析的方法，一般的分析都可以直接进行。 分析评估流程 整个分析流程并不复杂，主要是前期对数据库的整理，特别是对数据库的注释文件的检查和包含注释的信息的熟悉，选择你需要的信息。 有整合在一起的pipeline包含了组装、基因预测和注释的功能，比如Bactopia，操作方便，只是功能都是固定好的。当然你也可以直接用其基因预测的结果，因为该pipeline使用到的软件也是Prokka. 我把具体的操作分析流程分步展示如下： 1. 序列组装 完成图：二代+三代测序数据 # -1，-2 表示二代短读长测序的双端测序数据，-l 表示三代长读长测序数据，-o 表示输出文件夹 unicycler -1 test_1.fq.gz -2 test_2.fq.gz -l test_long.fq.gz -o Assembly -t 16 扫描图：二代测序数据 unicycler -1 test_1.fq.gz -2 test_2.fq.gz -o Assembly -t 16 主要输出结果说明： Assembly/assembly.fasta 组装结果的fasta格式，下一步分析使用的 Assembly/assembly.gfa 组装结果的gfa格式，特别是对于扫描图基因组来说，多个contig的连接关系可以很清晰地看到，利用 Bandage 可以看到 Assembly/unicycler.log 组装过程日志，你可以看到unicycler在组装过程中到底做了哪些事情 2. 基因组物种分类鉴定 GTDBtk # --genome_dir 基因组存放的文件夹，--out_dir 指定输出文件夹，--prefix 指定输出文件的前缀，--extension 基因组的后缀 gtdbtk classify_wf --genome_dir Assembly --out_dir GTDB_result --cpus 16 --prefix test --extension fasta GTDBtk主要输出结果说明： GTDB_result/test.bac120.summary.tsv 物种分类结果, 包括物种分类信息，与参考基因组的ANI相似度等具体信息 PGAP # -o 表示输出文件夹，-g 表示输入的基因组，-s 表示提供的物种信息，是程序运行的必须信息，你可以利用GTDBtk运行得到的分类信息，使用的taxcheck参数会检查你提供的物种信息，如果不一致会自动纠正，--no-internet 表示不使用互联网，--no-self-update 表示不更新，--taxcheck 表示检查物种分类信息，会得到checkM的分析结果，--auto-correct-tax 表示自动纠正物种分类信息，--ignore-all-errors 表示忽略运行中的其他错误 pgap.py -r -o PGAP_result -g Assembly/assembly.fasta -s 'Bifidobacterium longum' --no-internet --no-self-update --taxcheck --auto-correct-tax --ignore-all-errors PGAP主要输出结果说明： 你可以点击 这个链接 来查看输出结果文件夹下每个文件的含义 最重要的文件的结果是：PGAP_result/ani-tax-report.txt，是物种检查的结果，点击 这个链接 查看详细说明 3. 基因预测 # --outdir 输出文件夹，--prefix 输出文件的前缀，--kingdom 物种分类信息，--rfam 是否使用rfam数据库 prokka --outdir Prokka --force --prefix test --kingdom Bacteria --rfam --cpus 16 Assembly/assembly.fasta Prokka输出结果说明： prokka输出的就是基因的相关信息，比如核苷酸序列，CDS的蛋白序列，gff/gbk等基因注释文件，以及对基因统计的表格。具体的说明，点击 这个链接 来查看 Prokka/prokka.faa CDS的蛋白序列 Prokka/prokka.ffn 核苷酸序列 4. 基因注释 这里的基因注释指的是用抗生素耐药性基因数据库、毒力因子数据库和可移动元件的注释。对于rRNA、tRNA以及其他基因的注释，Prokka已经分析了，其底层利用的数据库是UniProt, Pfam和TIGRFAMs。当然Prokka还会预测到很多假定蛋白，这些基因可能是某些功能基因，注释需要用其他数据库来注释。 CARD-RGI抗生素耐药性基因注释 CARD抗生素耐药性基因注释通过RGI的方式去分析，而不是直接利用BLAST和DIAMOND的方式去分析。 # --input_sequence 输入的基因序列，--output_file 输出文件的前缀，--local 本地运行 -t 基因类型 rgi main --input_sequence Prokka/prokka.faa --output_file Card_result --local --clean -t protein DIAMOND比对 如果数据库提供了蛋白序列，则利用diamond比对的方式去做，比blastp的方式要快一些。这里以VFDB毒力因子数据库为例， # 数据库构建diamond的索引，--in 输入的需要构建索引的蛋白序列，--db 输出的索引 diamond makedb --in VFDB_setA_pro.fas --db VFDB_setA_pro # 利用diamond blastp去比对，--db 数据库索引，--query 待比对的蛋白序列，--out 比对输出结果，--evalue 比对evalue阈值，--max-target-seqs 最大比对目标序列数，--outfmt 输出格式 diamond blastp --db VFDB_setA_pro.dmnd --query Prokka/prokka.faa --out vf_anno.txt --evalue 0.00001 --max-target-seqs 1 --outfmt 6 qseqid sseqid pident length qcovhsp mismatch gapopen qstart qend sstart send evalue bitscore BLAST比对 有的数据库只提供了核苷酸序列，则用blastn比对的方式去做。这里以MEGARes耐药基因数据库为例， # 数据库构建索引，-dbtype 数据库类型(nucl还是prot)，-in 输入的序列，-input_type 序列类型，-out 输出的索引 makeblastdb -dbtype nucl -in megares_database_v3.00.fasta -input_type fasta -out megares # blastn比对，-query 待比对的核苷酸序列，-db 数据库索引，-out 比对输出结果，-evalue 比对evalue阈值，-max_target_seqs 最大比对目标序列数，-max_hsp 比对最大的hsp序列数，-outfmt 输出格式 blastn -query Prokka/prokka.ffn -db megares -out megares_anno.txt -evalue 0.00001 -max_target_seqs 1 -max_hsps 1 -outfmt '6 qseqid sseqid pident length qcovhsp mismatch gapopen qstart qend sstart send evalue bitscore' 5. 后续其他分析 可移动元件分析，也是利用预测得到的基因和可移动元件数据库mobileOG-db进行比对的方式获得的，为了确定抗生素耐药性基因是不是由于水平基因转移的方式获得的，只需要看下抗生素耐药性基因编号和可移动元件的编号是不是对应的即可。具体的你可以参考这篇文章。 做完基因注释就得到了主要的分析结果了。为了后续便于查询和可视化的操作，后续我利用Proksee来绘制基因组圈图，并用Rmd的方式得到一个html形式的报告。 ",
      "url"      : "http://localhost:4000/2023/09/14/pipeline-bacterial-genome/",
      "keywords" : "细菌基因组, 临床数据分析"
    } ,
  
    {
      "title"    : "QIIME2 构建数据库以及常规分析流程",
      "category" : "测序数据分析",
      "content": "QIIME2 分析流程是扩增子分析常用的流程之一，这里提供一个完整的 16S 数据分析流程，包括构建数据库以及常规分析。QIIME2 是各种插件组成的综合体 pipeline，具体的信息可以看官方网站 QIIME2. 构建数据库的过程，参考：Processing, filtering, and evaluating the SILVA database with RESCRIPt 下载 SILVA 数据库 在 SILVA 数据库下载并解压如下文件：  tax_slv_ssu_138.1.txt taxmap_slv_ssu_ref_nr_138.1.txt tax_slv_ssu_138.1.tre SILVA_138.1_SSURef_NR99_tax_silva_trunc.fasta 初步构建 QIIME2 物种分类数据库 Import the Taxonomy Rank file qiime tools import     --type 'FeatureData[SILVATaxonomy]'     --input-path tax_slv_ssu_138.1.txt     --output-path taxranks-silva-138.1-ssu-nr99.qza Import the Taxonomy Mapping file qiime tools import     --type 'FeatureData[SILVATaxidMap]'     --input-path taxmap_slv_ssu_ref_nr_138.1.txt     --output-path taxmap-silva-138.1-ssu-nr99.qza Import the Taxonomy Hierarchy Tree file qiime tools import     --type 'Phylogeny[Rooted]'     --input-path tax_slv_ssu_138.1.tre     --output-path taxtree-silva-138.1-nr99.qza Import the sequence file qiime tools import     --type 'FeatureData[RNASequence]'     --input-path SILVA_138.1_SSURef_NR99_tax_silva_trunc.fasta     --output-path silva-138.1-ssu-nr99-rna-seqs.qza 转换 FeatureData[RNASequence] 为 FeatureData[DNASequence]，获得物种序列 qiime rescript reverse-transcribe     --i-rna-sequences silva-138.1-ssu-nr99-rna-seqs.qza     --o-dna-sequences silva-138.1-ssu-nr99-seqs.qza 使用 qiime2 的 silva 分类法创建物种标签分类 qiime rescript parse-silva-taxonomy     --i-taxonomy-tree taxtree-silva-138.1-nr99.qza     --i-taxonomy-map taxmap-silva-138.1-ssu-nr99.qza     --i-taxonomy-ranks taxranks-silva-138.1-ssu-nr99.qza     --o-taxonomy silva-138.1-ssu-nr99-tax.qza ——————————————- Optional (可选的) ——————————————- 也可以直接下载别人构建好的数据库来使用 qiime rescript get-silva-data     --p-version '138.1'     --p-target 'SSURef_NR99'     --o-silva-sequences silva-138.1-ssu-nr99-rna-seqs.qza     --o-silva-taxonomy silva-138.1-ssu-nr99-tax.qza qiime rescript reverse-transcribe     --i-rna-sequences silva-138.1-ssu-nr99-rna-seqs.qza  --o-dna-sequences silva-138.1-ssu-nr99-seqs.qza ——————————————- Optional (可选的) ——————————————- 对初步数据库进行质控 删除有 5 个以上模糊碱基的序列，8 个碱基以上的同聚物（Homopolymer 是指基因组上单一碱基重复的区域） # 这里用的是默认参数 qiime rescript cull-seqs     --i-sequences silva-138.1-ssu-nr99-seqs.qza     --o-clean-sequences silva-138.1-ssu-nr99-seqs-cleaned.qza 通过序列长度和物种分类信息过滤数据库中的序列，具体地，Archaea (16S) &gt;= 900 bp, Bacteria (16S) &gt;= 1200 bp, and any Eukaryota (18S) &gt;= 1400 bp. qiime rescript filter-seqs-length-by-taxon     --i-sequences silva-138.1-ssu-nr99-seqs-cleaned.qza     --i-taxonomy silva-138.1-ssu-nr99-tax.qza     --p-labels Archaea Bacteria Eukaryota     --p-min-lens 900 1200 1400     --o-filtered-seqs silva-138.1-ssu-nr99-seqs-filt.qza     --o-discarded-seqs silva-138.1-ssu-nr99-seqs-discard.qza 去除重复冗余的序列 qiime rescript dereplicate     --i-sequences silva-138.1-ssu-nr99-seqs-filt.qza     --i-taxa silva-138.1-ssu-nr99-tax.qza     --p-mode 'uniq'     --o-dereplicated-sequences silva-138.1-ssu-nr99-seqs-derep-uniq.qza     --o-dereplicated-taxa silva-138.1-ssu-nr99-tax-derep-uniq.qza 构建物种分类器 经过上述处理后，构建 bayes 物种分类器，当然也有其他算法的分类器，可以看 QIIME2 官网，这里的分类器基于的 silva 数据库中 16S 全长序列（部分可能不是）。输出的 silva-138.1-ssu-nr99-classifier.qza 是后续分析可以用到的。 qiime feature-classifier fit-classifier-naive-bayes    --i-reference-reads silva-138.1-ssu-nr99-seqs-derep-uniq.qza    --i-reference-taxonomy silva-138.1-ssu-nr99-tax-derep-uniq.qza    --o-classifier silva-138.1-ssu-nr99-classifier.qza ——————————————- Optional (可选的) ——————————————- 上述步骤基于的是 16S 全长数据库，可以基于测序区间不同，来构建所需要引物区间的物种分类器，比如 515f-806r qiime feature-classifier extract-reads     --i-sequences silva-138.1-ssu-nr99-seqs-derep-uniq.qza     --p-f-primer GTGYCAGCMGCCGCGGTAA     --p-r-primer GGACTACNVGGGTWTCTAAT     --p-n-jobs 2     --p-read-orientation 'forward'     --o-reads silva-138.1-ssu-nr99-seqs-515f-806r.qza 在构建所需引物区间后还会再进行去重，固定引物区间后还会有可能有重复的 qiime rescript dereplicate     --i-sequences silva-138.1-ssu-nr99-seqs-515f-806r.qza     --i-taxa silva-138.1-ssu-nr99-tax-derep-uniq.qza     --p-mode 'uniq'     --o-dereplicated-sequences silva-138.1-ssu-nr99-seqs-515f-806r-uniq.qza     --o-dereplicated-taxa silva-138.1-ssu-nr99-tax-515f-806r-derep-uniq.qza 在上述基础上，构建最终的 515f-806r 分类器 qiime feature-classifier fit-classifier-naive-bayes     --i-reference-reads silva-138.1-ssu-nr99-seqs-515f-806r-uniq.qza     --i-reference-taxonomy silva-138.1-ssu-nr99-tax-515f-806r-derep-uniq.qza     --o-classifier silva-138.1-ssu-nr99-515f-806r-classifier.qza ——————————————- Optional (可选的) ——————————————- 数据分析过程 分析前需要表明的是：扩增子数据测序方式和类型有多种，例如 pair-end 双端测序、single-end 单端测序、单一区间测序、16S 全长测序等等，以及测序公司给你返回的原始数据中包不包含引物序列等等，这都需要根据自己的测序数据来具体分析。 这里提供的分析过程是最常见的一种，双端测序、测序公司去除了引物序列并进行了质控、过程只包括从 fq 数据到 ASV table，物种注释结果以及 ASV 序列。我想后续的分析内容无非是根据自己的实验分组情况，在不同物种分类水平做差异分析。 制作样本序列信息表 (例如名为 manifest.tsv)，3 列信息分别是样本名称和两端测序数据的路径，形式如下： sample-id  forward-absolute-filepath  reverse-absolute-filepath SRR12466539  /root/pipe_script/qiime2/test/SRR12466539/SRR12466539_1.fastq.gz  /root/pipe_script/qiime2/test/SRR12466539/SRR12466539_2.fastq.gz SRR12466540  /root/pipe_script/qiime2/test/SRR12466540/SRR12466540_1.fastq.gz  /root/pipe_script/qiime2/test/SRR12466540/SRR12466540_2.fastq.gz 将上一步数据读取 qiime tools import     --type 'SampleData[PairedEndSequencesWithQuality]'     --input-path /root/pipe_script/qiime2/test/manifest.tsv     --output-path result/paired-end-demux.qza     --input-format PairedEndFastqManifestPhred33V2 数据质量 summary 可视化 qiime demux summarize     --i-data result/paired-end-demux.qza     --o-visualization result/paired-end-demux-summary.qzv 利用 dada2 进行 denoise，我这里使用的是质控好的数据，所以几个质控参数设置的都是 0，即不对序列做处理，也就意味着用 denoise-single 和 denoise-paired 都没问题。 # --o-representative-sequences 输出的代表序列 # --o-table 输出的 ASV table # --o-denoising-stats 输出的 denoise 统计信息 qiime dada2 denoise-single     --i-demultiplexed-seqs result/paired-end-demux.qza     --p-trim-left 0     --p-trunc-len 0     --p-n-threads 8     --o-representative-sequences result/rep-seqs-dada2.qza     --o-table result/table-dada2.qza     --o-denoising-stats result/stats-dada2.qza 对 denoise 后的数据进行 summary 可视化 qiime feature-table summarize     --i-table result/table-dada2.qza     --o-visualization result/table-dada2.qzv 利用 bayes 方法做物种注释 # --o-classification 物种分类结果 # --i-classifier 就是上面构建好的物种分类器 qiime feature-classifier classify-sklearn     --i-classifier /root/database/SILVA_138.1_SSURef_NR99/silva-138.1-ssu-nr99-classifier.qza     --i-reads result/rep-seqs-dada2.qza     --o-classification result/taxonomy-dada2-sliva.qza     --p-n-jobs 3     --verbose 对上述 qza 结果转换输出为 tsv，fasta 格式文件。 # 输出统计信息表 qiime tools export     --input-path result/stats-dada2.qza     --output-path result/ # default out file: stats.tsv # 输出代表序列 qiime tools export     --input-path result/rep-seqs-dada2.qza     --output-path result/ # default out file: dna-sequences.fasta # 输出biom格式的ASV table qiime tools export     --input-path result/table-dada2.qza     --output-path result/ # default out file: feature-table.biom # 输出物种分类结果 qiime tools export     --input-path result/taxonomy-dada2-sliva.qza     --output-path result/ # default out file: taxonomy.tsv # 转换biom格式为tsv格式 biom convert -i feature-table.biom -o feature-table.tsv --to-tsv 经过上述步骤就得到了扩增子分析中重要的几个结果了，ASV table、ASV 序列和物种注释结果。后续的相关分析就是基于这几个结果去做。 ",
      "url"      : "http://localhost:4000/2023/10/27/pipeline-qiime2-analysis/",
      "keywords" : "QIIME2，测序数据分析"
    } ,
  
    {
      "title"    : "阿里云服务器 mysql 数据库为什么用 DBeaver 等软件远程连接不上？",
      "category" : "数据库",
      "content": "解决 DBeaver 远程连接阿里云 MySQL 数据库时出现“无法连接到数据库”、“时间超时”等问题。 这里解决的具体问题是，在 host、端口、用户、密码等信息设置正确的时候，DBeaver 连接的时候会报错“Communications link failure The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server. Can not read response from server. Expected to read 4 bytes, read 0 bytes before connectio”这样的信息。 查了很多博主的答案，大家都聚焦在 mysql 超时的问题，类似让修改 my.cnf 文件，增加 wait_timeout=31536000 这样的配置，也就是增加了连接超时时间。我按照这样的方法去尝试之后发现是行不通的，前前后后折腾了一会儿。由于自己薄弱的计算机网络知识。只能采取对比法排除，现在出问题的服务器是新开的，之前旧的服务器是能够正常连接的，所以对比了两个服务器的配置，发现新开的服务器的配置和旧的服务器配置是一模一样的，但是新开的服务器就是无法连接，而旧的服务器却可以正常连接。 最后发现是阿里云的安全组的问题。 首先，阿里云的安全组是默认关闭的，需要手动添加开启。 其次，阿里云的安全组默认只开两个端口，也就是 SSH 的 22 和 RDP 的 3389。 mysql 的端口默认是 3306，在你新开阿里云服务器后，mysql 的 3306 端口并没有开启，需要手动开启。 阿里云的安全组需要手动添加规则，添加规则的目的是允许从指定的 IP 地址访问指定的端口。阿里云服务器开放端口给 IP 使用，可以参考阿里云的官方文档：阿里云服务器开放全部端口给所有 IP 使用教程 在添加了 mysql 的端口后，DBeaver 就可以正常连接了。 由于自己薄弱的计算机网络知识，导致浪费了很多时间，不过最终找到了问题所在，也在这里记录一下，以备后用。 ",
      "url"      : "http://localhost:4000/2024/01/11/issue-aliyun-mysql/",
      "keywords" : "MySQL，数据库，阿里云"
    } ,
  
    {
      "title"    : "ascp",
      "category" : "",
      "content": "如何安装 ascp？ 直接推荐 conda 下载的方式： # 安装 conda create -n ascp conda activate ascp conda install -c hcc aspera-cli # 安装后id_rsa的位置为： /root/miniconda3/envs/ascp/etc/asperaweb_id_dsa.openssh 如何使用 ascp？ 一个简单的下载示例： ascp -i /root/miniconda3/envs/ascp/etc/asperaweb_id_dsa.openssh -l 100M -QT -P33001 -k1 era-fasp@fasp.sra.ebi.ac.uk:/vol1/fastq/SRR769/007/SRR7696207/SRR7696207_1.fastq.gz ./ 去哪里获取待下载数据的链接？ 我这里只推荐 ENA 数据库来获取数据下载链接 ENA ENA 提供了 SRA 数据和原始的 fq.gz 数据下载链接。比如下载 SRR7696207 的双端数据，在 ENA 中搜索 SRR7696207 后，选择显示 fastq_aspera 和 sra_aspera（默认是不显示的）。 然后点击对应的链接就可以复制 ascp 下载链接了，直接替换下载链接即可。 ",
      "url"      : "http://localhost:4000/wiki/ascp/",
      "keywords" : "ascp"
    } ,
  
    {
      "title"    : "MySQL 表空间碎片回收",
      "category" : "",
      "content": "从 MySQL 表中大量删除数据后，有可能表占用的空间并不会马上回收掉，此时如果在意空间占用，可以主动进行空间碎片回收。 查看表空间碎片占用 方法一： SELECT table_schema db,   table_name,   data_free,   engine FROM information_schema.tables  WHERE table_schema NOT IN ('information_schema', 'mysql')  AND data_free &gt; 0 and table_name = 'xxx' ORDER BY DATA_FREE DESC; 方法二： show table status like 'xxx'; data_free 字段对应的值就是碎片字节数。 表空间碎片回收 -- 适用 InnoDB 表 ALTER TABLE xxx engine = InnoDB 有一篇相关的文章讲得比较好可以参考：https://www.cnblogs.com/wanng/p/mysql-recycle-table-space.html ",
      "url"      : "http://localhost:4000/fragment/mysql-space-recover/",
      "keywords" : "MySQL"
    } ,
  
    {
      "title"    : "Fragment Template",
      "category" : "",
      "content": "Content here ",
      "url"      : "http://localhost:4000/fragment/template/",
      "keywords" : "keyword1, keyword2"
    } 
  
]

